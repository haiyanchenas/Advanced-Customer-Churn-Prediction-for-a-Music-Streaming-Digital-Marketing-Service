{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip -q install torch==2.0.1 torch-geometric==2.4.0 torch-scatter torch-sparse torch-cluster torch-spline-conv scikit-learn pandas numpy tqdm\n"
      ],
      "metadata": {
        "id": "7lDNzEm0HQOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMpRcOYz_eYJ"
      },
      "outputs": [],
      "source": [
        "# hybrid_gat_mlp_churn.py\n",
        "# pipeline for the WSDM–KKBox churn task.\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.nn import GATv2Conv, BatchNorm\n",
        "\n",
        "# -------------------------\n",
        "# CONFIG\n",
        "# -------------------------\n",
        "CFG = {\n",
        "    \"PATHS\": {\n",
        "        \"train\": \"/content/train_v2.csv\",\n",
        "        \"members\": \"/content/members_v3.csv\",\n",
        "        \"transactions\": \"/content/transactions_v2.csv\",\n",
        "        \"user_logs\": \"/content/user_logs_v2.csv\",  # ~30GB\n",
        "    },\n",
        "    \"RANDOM_SEED\": 42,\n",
        "    \"LOG_CHUNK_ROWS\": 1_000_000,     # chunk size for user_logs\n",
        "    \"MAX_CANCEL_CAP\": 4,\n",
        "    \"GRAPH_MAX_NEIGHBORS_PER_GROUP\": 10,  # K edges per user inside each (city, registered_via) group\n",
        "    \"BATCH_SIZE\": 512,\n",
        "    \"LR\": 5e-3,\n",
        "    \"WEIGHT_DECAY\": 1e-4,\n",
        "    \"MAX_EPOCHS\": 50,\n",
        "    \"EARLY_STOP_PATIENCE\": 5,\n",
        "    \"GAT_HIDDEN\": 32,\n",
        "    \"GAT_HEADS\": 4,\n",
        "    \"GAT_DROPOUT\": 0.6,\n",
        "    \"MLP_HIDDEN\": 64,\n",
        "    \"VAL_RATIO\": 0.15,\n",
        "    \"TEST_RATIO\": 0.15,\n",
        "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "}\n",
        "\n",
        "random.seed(CFG[\"RANDOM_SEED\"])\n",
        "np.random.seed(CFG[\"RANDOM_SEED\"])\n",
        "torch.manual_seed(CFG[\"RANDOM_SEED\"])\n",
        "\n",
        "# -------------------------\n",
        "# UTILITIES\n",
        "# -------------------------\n",
        "def parse_date(series):\n",
        "    return pd.to_datetime(series, errors=\"coerce\")\n",
        "\n",
        "def safe_int(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def dict_increment(d, key, val):\n",
        "    d[key] = d.get(key, 0) + val\n",
        "\n",
        "# -------------------------\n",
        "# 3.2 DATA PREPARATION\n",
        "# -------------------------\n",
        "def aggregate_transactions(path_transactions):\n",
        "    print(\"Aggregating transactions ...\")\n",
        "    tx = pd.read_csv(path_transactions)\n",
        "\n",
        "    # robust date parsing\n",
        "    for col in [\"transaction_date\", \"membership_expire_date\"]:\n",
        "        if col in tx.columns:\n",
        "            tx[col] = parse_date(tx[col])\n",
        "\n",
        "    # payment_amount might be missing/float\n",
        "    pay_col = \"payment_plan_price\"\n",
        "    if \"actual_amount_paid\" in tx.columns:\n",
        "        pay_col = \"actual_amount_paid\"  # KKBox has this sometimes\n",
        "\n",
        "    agg = tx.groupby(\"msno\").agg(\n",
        "        total_transactions=(\"msno\", \"size\"),\n",
        "        total_payment=(pay_col, \"sum\"),\n",
        "        is_cancel_sum=(\"is_cancel\", \"sum\"),\n",
        "        last_transaction_date=(\"transaction_date\", \"max\")\n",
        "    ).reset_index()\n",
        "\n",
        "    # cap cancels as your text suggests\n",
        "    agg[\"is_cancel_sum\"] = agg[\"is_cancel_sum\"].clip(upper=CFG[\"MAX_CANCEL_CAP\"]).fillna(0).astype(int)\n",
        "    agg[\"total_payment\"] = agg[\"total_payment\"].fillna(0.0)\n",
        "\n",
        "    return agg\n",
        "\n",
        "def aggregate_user_logs(path_logs, chunk_rows):\n",
        "    print(\"Aggregating user logs (chunked) ...\")\n",
        "    # Expected cols (typical KKBox): msno, date, num_25, num_50, num_75, num_985, num_100, num_unq, total_secs\n",
        "    agg_dict = {}  # msno -> dict of metrics\n",
        "\n",
        "    usecols = None  # None = auto\n",
        "    chunks = pd.read_csv(path_logs, chunksize=chunk_rows, iterator=True)\n",
        "    for chunk in tqdm(chunks, desc=\"user_logs chunks\"):\n",
        "        if \"date\" in chunk.columns:\n",
        "            chunk[\"date\"] = parse_date(chunk[\"date\"])\n",
        "        # coerce numeric\n",
        "        if \"total_secs\" in chunk.columns:\n",
        "            chunk[\"total_secs\"] = pd.to_numeric(chunk[\"total_secs\"], errors=\"coerce\").fillna(0)\n",
        "        if \"num_unq\" in chunk.columns:\n",
        "            chunk[\"num_unq\"] = pd.to_numeric(chunk[\"num_unq\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "        # aggregate at chunk level\n",
        "        g = chunk.groupby(\"msno\").agg(\n",
        "            log_days=(\"date\", \"nunique\") if \"date\" in chunk.columns else (\"msno\", \"size\"),\n",
        "            total_secs_sum=(\"total_secs\", \"sum\") if \"total_secs\" in chunk.columns else (\"msno\", \"size\"),\n",
        "            total_songs_played=(\"num_unq\", \"sum\") if \"num_unq\" in chunk.columns else (\"msno\", \"size\"),\n",
        "        )\n",
        "\n",
        "        # merge into dict\n",
        "        for msno, row in g.iterrows():\n",
        "            d = agg_dict.get(msno)\n",
        "            if d is None:\n",
        "                agg_dict[msno] = {\n",
        "                    \"log_days\": int(row[\"log_days\"]),\n",
        "                    \"total_secs_sum\": float(row[\"total_secs_sum\"]),\n",
        "                    \"total_songs_played\": float(row[\"total_songs_played\"]),\n",
        "                }\n",
        "            else:\n",
        "                d[\"log_days\"] += int(row[\"log_days\"])\n",
        "                d[\"total_secs_sum\"] += float(row[\"total_secs_sum\"])\n",
        "                d[\"total_songs_played\"] += float(row[\"total_songs_played\"])\n",
        "\n",
        "        del chunk, g\n",
        "        gc.collect()\n",
        "\n",
        "    # dict -> DataFrame\n",
        "    df = pd.DataFrame.from_dict(agg_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"msno\"})\n",
        "    # numeric types\n",
        "    df[\"log_days\"] = df[\"log_days\"].astype(int)\n",
        "    return df\n",
        "\n",
        "def load_members(path_members):\n",
        "    print(\"Loading members ...\")\n",
        "    mem = pd.read_csv(path_members)\n",
        "    # parse registration time\n",
        "    if \"registration_init_time\" in mem.columns:\n",
        "        # KKBox format is yyyymmdd integer\n",
        "        mem[\"registration_init_time\"] = pd.to_datetime(mem[\"registration_init_time\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "    # keep essential columns\n",
        "    keep = [\"msno\", \"city\", \"gender\", \"registered_via\", \"registration_init_time\"]\n",
        "    mem = mem[keep]\n",
        "    return mem\n",
        "\n",
        "def load_train_labels(path_train):\n",
        "    tr = pd.read_csv(path_train)[[\"msno\", \"is_churn\"]]\n",
        "    tr[\"is_churn\"] = tr[\"is_churn\"].fillna(0).astype(int)\n",
        "    return tr\n",
        "\n",
        "def build_user_level_table(paths):\n",
        "    tx_agg = aggregate_transactions(paths[\"transactions\"])\n",
        "    logs_agg = aggregate_user_logs(paths[\"user_logs\"], CFG[\"LOG_CHUNK_ROWS\"])\n",
        "    members = load_members(paths[\"members\"])\n",
        "    labels = load_train_labels(paths[\"train\"])\n",
        "\n",
        "    print(\"Merging all sources ...\")\n",
        "    df = labels.merge(members, on=\"msno\", how=\"left\") \\\n",
        "               .merge(tx_agg, on=\"msno\", how=\"left\") \\\n",
        "               .merge(logs_agg, on=\"msno\", how=\"left\")\n",
        "\n",
        "    # fill NaNs with zeros as per your write-up\n",
        "    num_cols = [\"total_transactions\", \"total_payment\", \"is_cancel_sum\",\n",
        "                \"log_days\", \"total_secs_sum\", \"total_songs_played\"]\n",
        "    for c in num_cols:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "    # 3.3 FEATURE ENGINEERING\n",
        "    print(\"Feature engineering ...\")\n",
        "    df[\"last_transaction_date\"] = parse_date(df[\"last_transaction_date\"])\n",
        "    df[\"registration_init_time\"] = parse_date(df[\"registration_init_time\"])\n",
        "\n",
        "    # membership_days (difference in days)\n",
        "    df[\"membership_days\"] = (\n",
        "        (df[\"last_transaction_date\"] - df[\"registration_init_time\"]).dt.days\n",
        "    )\n",
        "    df[\"membership_days\"] = df[\"membership_days\"].fillna(0).clip(lower=0)\n",
        "\n",
        "    # registration_year/month\n",
        "    df[\"registration_year\"] = df[\"registration_init_time\"].dt.year.fillna(0).astype(int)\n",
        "    df[\"registration_month\"] = df[\"registration_init_time\"].dt.month.fillna(0).astype(int)\n",
        "\n",
        "    # label encode low-card categorical vars\n",
        "    for col in [\"city\", \"gender\", \"registered_via\"]:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = df[col].fillna(\"unknown\")\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "    # finalize feature set\n",
        "    feature_cols = [\n",
        "        \"city\", \"gender\", \"registered_via\",\n",
        "        \"total_transactions\", \"total_payment\", \"is_cancel_sum\",\n",
        "        \"log_days\", \"total_secs_sum\", \"total_songs_played\",\n",
        "        \"membership_days\", \"registration_year\", \"registration_month\"\n",
        "    ]\n",
        "    # keep 15-ish features: add a few simple interactions if needed\n",
        "    # (You documented 15D; depending on columns here you can add derived ones)\n",
        "    # For consistency, we'll add 3 more simple transforms:\n",
        "    df[\"avg_secs_per_day\"] = (df[\"total_secs_sum\"] / (df[\"log_days\"] + 1e-6)).fillna(0)\n",
        "    df[\"avg_songs_per_day\"] = (df[\"total_songs_played\"] / (df[\"log_days\"] + 1e-6)).fillna(0)\n",
        "    df[\"pay_per_tx\"] = (df[\"total_payment\"] / (df[\"total_transactions\"] + 1e-6)).fillna(0)\n",
        "\n",
        "    feature_cols.extend([\"avg_secs_per_day\", \"avg_songs_per_day\", \"pay_per_tx\"])\n",
        "\n",
        "    # scaling (Z-score)\n",
        "    print(\"Scaling numeric features ...\")\n",
        "    scaler = StandardScaler()\n",
        "    df[feature_cols] = scaler.fit_transform(df[feature_cols].astype(float))\n",
        "\n",
        "    return df, feature_cols, scaler\n",
        "\n",
        "# -------------------------\n",
        "# GRAPH CONSTRUCTION (Eq.4)\n",
        "# -------------------------\n",
        "def build_sparse_similarity_edges(df, max_neighbors=10):\n",
        "    \"\"\"\n",
        "    Create edges for users that share BOTH (city, registered_via).\n",
        "    To stay sparse on ~1M nodes, we connect each user to up to K peers\n",
        "    within its group by linking to the next K users in the group's index order.\n",
        "    \"\"\"\n",
        "    print(\"Building sparse similarity graph ...\")\n",
        "    df = df.reset_index(drop=True)\n",
        "    df[\"node_id\"] = np.arange(len(df))\n",
        "    group_key = list([\"city\", \"registered_via\"])\n",
        "\n",
        "    edges_src = []\n",
        "    edges_dst = []\n",
        "\n",
        "    for _, g in tqdm(df.groupby(group_key), total=df.groupby(group_key).ngroups):\n",
        "        ids = g[\"node_id\"].to_numpy()\n",
        "        if len(ids) <= 1:\n",
        "            continue\n",
        "        # wire to up to K neighbors (directed edges both ways)\n",
        "        K = min(max_neighbors, len(ids) - 1)\n",
        "        # simple ring neighbors to keep degree bounded and graph connected inside group\n",
        "        for i in range(len(ids)):\n",
        "            for k in range(1, K + 1):\n",
        "                j = (i + k) % len(ids)\n",
        "                edges_src.append(ids[i])\n",
        "                edges_dst.append(ids[j])\n",
        "                edges_src.append(ids[j])\n",
        "                edges_dst.append(ids[i])\n",
        "\n",
        "    edge_index = torch.tensor([edges_src, edges_dst], dtype=torch.long)\n",
        "    print(f\"Edges built: {edge_index.size(1):,}\")\n",
        "    return edge_index, df[\"node_id\"].values\n",
        "\n",
        "# -------------------------\n",
        "# MODEL: GAT + MLP Hybrid\n",
        "# -------------------------\n",
        "class HybridGATMLP(nn.Module):\n",
        "    def __init__(self, in_dim_tabular, gat_in_dim, gat_hidden=32, gat_heads=4,\n",
        "                 gat_dropout=0.6, mlp_hidden=64):\n",
        "        super().__init__()\n",
        "        # GAT encoder\n",
        "        self.gat1 = GATv2Conv(gat_in_dim, gat_hidden, heads=gat_heads, dropout=gat_dropout, concat=True)\n",
        "        self.bn1 = BatchNorm(gat_hidden * gat_heads)\n",
        "        self.gat2 = GATv2Conv(gat_hidden * gat_heads, gat_hidden, heads=1, dropout=gat_dropout, concat=True)\n",
        "        self.bn2 = BatchNorm(gat_hidden)\n",
        "\n",
        "        # MLP path on tabular (assume same as input to GAT here; if different, pass both)\n",
        "        self.fc1 = nn.Linear(in_dim_tabular, mlp_hidden)\n",
        "        self.bn_tab1 = nn.BatchNorm1d(mlp_hidden)\n",
        "        self.fc2 = nn.Linear(mlp_hidden, in_dim_tabular)  # project back to ~15 dims as in your text\n",
        "        self.bn_tab2 = nn.BatchNorm1d(in_dim_tabular)\n",
        "\n",
        "        self.dropout = nn.Dropout(gat_dropout)\n",
        "\n",
        "        fused_dim = gat_hidden + in_dim_tabular  # z_i (32) + t2 (15≈in_dim_tabular)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fused_dim, fused_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(fused_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_tab, x_gat, edge_index):\n",
        "        # GAT path\n",
        "        z = self.gat1(x_gat, edge_index)\n",
        "        z = F.elu(self.bn1(z))\n",
        "        z = self.dropout(z)\n",
        "        z = self.gat2(z, edge_index)\n",
        "        z = F.elu(self.bn2(z))\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # MLP path\n",
        "        t = self.fc1(x_tab)\n",
        "        t = F.relu(self.bn_tab1(t))\n",
        "        t = self.dropout(t)\n",
        "        t = self.fc2(t)\n",
        "        t = F.relu(self.bn_tab2(t))\n",
        "        t = self.dropout(t)\n",
        "\n",
        "        u = torch.cat([z, t], dim=1)\n",
        "        logits = self.classifier(u).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# TRAIN / EVAL\n",
        "# -------------------------\n",
        "def train_one_epoch(model, loader, optimizer, pos_weight, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch.x_tab, batch.x_gat, batch.edge_index)\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, batch.y.float(), pos_weight=pos_weight)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss.item()) * batch.num_nodes\n",
        "        total += batch.num_nodes\n",
        "    return total_loss / max(1, total)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        logits = model(batch.x_tab, batch.x_gat, batch.edge_index)\n",
        "        prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        y = batch.y.cpu().numpy()\n",
        "        ys.append(y)\n",
        "        ps.append(prob)\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return auc, f1, acc\n",
        "\n",
        "# -------------------------\n",
        "# BUILD GRAPH DATA OBJECT\n",
        "# -------------------------\n",
        "def build_pyg_data(df, feature_cols, edge_index):\n",
        "    # features for both paths are from the same standardized tabular features\n",
        "    X = torch.tensor(df[feature_cols].values, dtype=torch.float32)\n",
        "    y = torch.tensor(df[\"is_churn\"].values, dtype=torch.long)\n",
        "\n",
        "    data = Data()\n",
        "    data.x_tab = X.clone()         # for MLP path\n",
        "    data.x_gat = X.clone()         # for GAT path (you can choose a subset if desired)\n",
        "    data.y = y\n",
        "    data.edge_index = edge_index\n",
        "    data.num_nodes = X.size(0)\n",
        "    return data\n",
        "\n",
        "# -------------------------\n",
        "# MAIN\n",
        "# -------------------------\n",
        "def main():\n",
        "    df, feature_cols, scaler = build_user_level_table(CFG[\"PATHS\"])\n",
        "\n",
        "    print(f\"Users after merge: {len(df):,}\")\n",
        "    print(\"Constructing similarity edges ...\")\n",
        "    edge_index, node_index = build_sparse_similarity_edges(\n",
        "        df[[\"city\", \"registered_via\"]].copy(),\n",
        "        max_neighbors=CFG[\"GRAPH_MAX_NEIGHBORS_PER_GROUP\"]\n",
        "    )\n",
        "\n",
        "    # Build torch-geometric Data\n",
        "    data = build_pyg_data(df, feature_cols, edge_index)\n",
        "\n",
        "    # Train/Val/Test split (stratified on y)\n",
        "    idx = np.arange(data.num_nodes)\n",
        "    train_idx, test_idx = train_test_split(idx, test_size=CFG[\"TEST_RATIO\"],\n",
        "                                           stratify=df[\"is_churn\"], random_state=CFG[\"RANDOM_SEED\"])\n",
        "    train_idx, val_idx = train_test_split(train_idx, test_size=CFG[\"VAL_RATIO\"]/(1-CFG[\"TEST_RATIO\"]),\n",
        "                                          stratify=df[\"is_churn\"].iloc[train_idx], random_state=CFG[\"RANDOM_SEED\"])\n",
        "\n",
        "    # masks\n",
        "    data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "    data.train_mask[torch.tensor(train_idx)] = True\n",
        "    data.val_mask[torch.tensor(val_idx)] = True\n",
        "    data.test_mask[torch.tensor(test_idx)] = True\n",
        "\n",
        "    # Subgraph loaders (NeighborLoader keeps graph structure)\n",
        "    train_loader = NeighborLoader(\n",
        "        data, num_neighbors=[15, 10], batch_size=CFG[\"BATCH_SIZE\"], input_nodes=data.train_mask\n",
        "    )\n",
        "    val_loader = NeighborLoader(\n",
        "        data, num_neighbors=[15, 10], batch_size=CFG[\"BATCH_SIZE\"], input_nodes=data.val_mask\n",
        "    )\n",
        "    test_loader = NeighborLoader(\n",
        "        data, num_neighbors=[15, 10], batch_size=CFG[\"BATCH_SIZE\"], input_nodes=data.test_mask\n",
        "    )\n",
        "\n",
        "    device = CFG[\"DEVICE\"]\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    model = HybridGATMLP(\n",
        "        in_dim_tabular=len(feature_cols),\n",
        "        gat_in_dim=len(feature_cols),\n",
        "        gat_hidden=CFG[\"GAT_HIDDEN\"],\n",
        "        gat_heads=CFG[\"GAT_HEADS\"],\n",
        "        gat_dropout=CFG[\"GAT_DROPOUT\"],\n",
        "        mlp_hidden=CFG[\"MLP_HIDDEN\"],\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"LR\"], weight_decay=CFG[\"WEIGHT_DECAY\"])\n",
        "\n",
        "    # class weights: w1=10 (churn), w0=1\n",
        "    pos_weight = torch.tensor([10.0], device=device)\n",
        "\n",
        "    # Early stopping\n",
        "    best_auc = -1.0\n",
        "    best_state = None\n",
        "    patience = CFG[\"EARLY_STOP_PATIENCE\"]\n",
        "    bad = 0\n",
        "\n",
        "    for epoch in range(1, CFG[\"MAX_EPOCHS\"] + 1):\n",
        "        tr_loss = train_one_epoch(model, train_loader, optimizer, pos_weight, device)\n",
        "        val_auc, val_f1, val_acc = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch:02d} | loss {tr_loss:.4f} | val AUC {val_auc:.4f} | F1 {val_f1:.4f} | Acc {val_acc:.4f}\")\n",
        "\n",
        "        if val_auc > best_auc + 1e-4:\n",
        "            best_auc = val_auc\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "\n",
        "    # Final test\n",
        "    test_auc, test_f1, test_acc = evaluate(model, test_loader, device)\n",
        "    print(f\"TEST | AUC {test_auc:.4f} | F1 {test_f1:.4f} | Acc {test_acc:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EVALUTION REPORTS\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------\n",
        "# EXTENDED EVALUATION\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def full_evaluation(model, loader, device, title=\"Test Set\"):\n",
        "    model.eval()\n",
        "    y_true, y_prob = [], []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        logits = model(batch.x_tab, batch.x_gat, batch.edge_index)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        y_prob.extend(probs)\n",
        "        y_true.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    # Classification metrics\n",
        "    print(f\"\\n=== {title} Metrics ===\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    print(f\"Accuracy: {acc:.4f} | AUC: {auc:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                xticklabels=[\"Not Churn\", \"Churn\"],\n",
        "                yticklabels=[\"Not Churn\", \"Churn\"])\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(f\"{title} – Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    return acc, auc, cm\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# LOSS & METRIC CURVES\n",
        "# -------------------------\n",
        "def plot_training_curves(train_losses, val_aucs, val_losses=None):\n",
        "    epochs = np.arange(1, len(train_losses)+1)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
        "    if val_losses:\n",
        "        plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training & Validation Loss Curves\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    if val_aucs:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(epochs, val_aucs, label=\"Val AUC\", color=\"green\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"AUC\")\n",
        "        plt.title(\"Validation AUC Curve\")\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "vZkRd05ZG4Fg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}